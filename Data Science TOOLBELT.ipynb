{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data_Science_Workflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACQUIRE THE DATA (CSV/HTML/SQL, API, Webscrape, Mongo, etc) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV/EXCEL/JSON/HTML/SQL #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "\n",
    "# Data Input and Output\n",
    "# type pwd to get location of current file\n",
    "\n",
    "# CSV\n",
    "df_csv = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names=['label','message'])\n",
    "df_csv.to_csv('My_output', index=False)  # Index=False keeps your same indexes, otherwise it will use index as first column\n",
    "\n",
    "# Excel\n",
    "df_excel = pd.read_excel('Excel_Sample.xlsx', sheetname='Sheet1')\n",
    "df_excel.to_excel('Excel_Sample2.xlsx',sheet_name='NewSheet')\n",
    "\n",
    "# JSON\n",
    "\n",
    "# HTML\n",
    "listy_HTML = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')  # By default creates a list of tables\n",
    "df_HTML = listy_HTML[0]\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "engine_sqlite = create_engine('sqlite:///:memory:')\n",
    "engine_postgres = create_engine('postgres://mfnyeqfz:zxzi25vFYOiloQ8ORKUo2-Oog5GgfZc2@elmer.db.elephantsql.com:5432/mfnyeqfz'\n",
    "df_csv.to_sql('my_table', engine_sqlite)\n",
    "df_sql = pd.read_sql('my_table', con=engine_sqlite)\n",
    "select = pd.read_sql(\"SELECT a,b FROM my_table WHERE d > 7\", con=engine_sqlite)\n",
    "\n",
    "engine = create_engine('postgres://mfnyeqfz:zxzi25vFYOiloQ8ORKUo2-Oog5GgfZc2@elmer.db.elephantsql.com:5432/mfnyeqfz')\n",
    "pd.read_sql(\"SELECT * FROM information_schema.tables;\", engine)[:3]\n",
    "pd.read_sql('SELECT released, title, '\n",
    "                '( CASE '\n",
    "                '  WHEN substring(released,6,2) in (\\'12\\', \\'01\\',\\'02\\') THEN \\'winter\\' '\n",
    "                '  WHEN substring(released,6,2) in (\\'03\\', \\'04\\',\\'05\\') THEN \\'spring\\' '    \n",
    "                '  WHEN substring(released,6,2) in (\\'06\\', \\'07\\',\\'08\\') THEN \\'summer\\' '\n",
    "                '  WHEN substring(released,6,2) in (\\'09\\', \\'10\\',\\'11\\') THEN \\'fall\\' ' \n",
    "                '  ELSE NULL'\n",
    "                '  END) AS season ' \n",
    "            'FROM website_imdb LIMIT 5 '\n",
    "    '', engine)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Built in Python\n",
    "\n",
    "import csv\n",
    "\n",
    "def open_with_csv(filename, d='\\t'):\n",
    "    data = []\n",
    "    with open(filename, mode='U') as tsvin:\n",
    "        tie_reader = csv.reader(tsvin, delimiter=d)\n",
    "        for line in tie_reader:\n",
    "            data.append(line)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data_from_csv = open_with_csv(filename='data.csv')\n",
    "#print(data_from_csv[0])\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# NumPy\n",
    "\n",
    "import numpy as np\n",
    "FIELDNAMES = data_from_csv[0]\n",
    "DATATYPES = [('myint', 'i'), ('myid', 'i'), ('price', 'f8'), ('name', 'a200'), ('brandId', '<i8'), \n",
    "             ('brandName', 'a200'), ('imageURL', '|S500'), ('description', '|S900'), ('vendor', '|S100'),\n",
    "             ('pattern', '|S50'), ('material', '|S50')]\n",
    "\n",
    "def load_data(filename, d='\\t'):\n",
    "    my_csv = np.genfromtxt(filename, delimiter=d, skip_header=1, invalid_raise=False, names=FIELDNAMES, dtype=DATATYPES)\n",
    "    return my_csv\n",
    "\n",
    "my_csv = load_data('data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import requests\n",
    "\n",
    "api_url = 'https://www.haloapi.com/stats/h5/players/madballa55/matches?modes=arena'\n",
    "headers = {'Ocp-Apim-Subscription-Key': 'bc640eada1bc4b079764c307d91db756'}\n",
    "\n",
    "response = requests.get(api_url, params=None, headers=headers)\n",
    "response.status_code\n",
    "match_history_JSON = json.loads(response.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading JSON Dictionary via API\n",
    "url = urllib2.urlopen('http://api.giphy.com/v1/gifs/search?q={funny+cat}&api_key=dc6zaTOxFJmzC').read()\n",
    "JSON_Giphy = json.loads(url)\n",
    "df_imdb = pd.DataFrame(list_of_dicty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo-DB #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://brownmt3255:dsi3255@ds031597.mlab.com:31597/browndb')  \n",
    "#dsi3255 = password. #database name = brown\n",
    "\n",
    "db_nba = client.browndb\n",
    "\n",
    "# Function to find what collections (tables) are in our DB\n",
    "def print_tables(db):\n",
    "    list_of_tables = []\n",
    "    collection = db.collection_names(include_system_collections=False)\n",
    "    for collect in collection:\n",
    "        list_of_tables.append(collect)\n",
    "    print \"List of Tables: \", list_of_tables\n",
    "\n",
    "# Name collection (table)\n",
    "nba_champ_collection = db_nba.test\n",
    "\n",
    "# This removes everything from the collection(table).\n",
    "nba_champ_collection.remove({})\n",
    "\n",
    "# Insert rows into collection (table)\n",
    "for x in listy:\n",
    "    nba_champ_collection.insert_one(x) \n",
    "\n",
    "# Create DF\n",
    "df_nba = pd.DataFrame(list(nba_champ_collection.find()))\n",
    "\n",
    "# See is what is inside the collection and DF\n",
    "for x in nba_champ_collection.find({})[0:5]:\n",
    "    print x\n",
    "\n",
    "df_nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading JSON Dictionary via public JSON (not API)\n",
    "list_of_dicty = []\n",
    "\n",
    "for movie_id in listy_movie_ids:\n",
    "    soup = BeautifulSoup(urllib2.urlopen('http://www.omdbapi.com/?i='+ movie_id + '&plot=short&r=json').read())\n",
    "    parsed_json = json.loads(soup.text)\n",
    "    \n",
    "    list_of_dicty.append(parsed_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Request #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chron Jobs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chron jobs use something like heroku scheduler or your own timing script to call this tweet_chron.py\n",
    "def handle(self, *args, **options):\n",
    "    for q in options['query']:\n",
    "        self.stdout.write(q)\n",
    "        try:\n",
    "            # Give twitter the authentication stuff\n",
    "            twitter = Twython(TWITTER_APP_KEY, TWITTER_APP_SECRET, TWITTER_OAUTH_TOKEN, TWITTER_OAUTH_TOKEN_SECRET)\n",
    "            tweets = []\n",
    "            # Query twitter for stuff\n",
    "            results = twitter.search(q='trump', result_type='recent')\n",
    "            \n",
    "            # This is me trying to make my database cache expire and refresh\n",
    "#             if TWITTERTWEET.objects.all().count() > 200:\n",
    "#                 tweet_models = TWITTERTWEET.objects.all()\n",
    "#                 times = []\n",
    "#                 for x in len(tweet_models):\n",
    "#                     times.append(x['created_at'])\n",
    "\n",
    "#                 for x in len(results):\n",
    "#                     m = min(times)\n",
    "#                     times.remove(min(mylist))\n",
    "#                     TWITTERTWEET.objects.filter(created_at=m).delete()\n",
    "\n",
    "            # Put the stuff in the database\n",
    "            for x in results['statuses']:\n",
    "                tweets += [TWITTERTWEET(\n",
    "                    text = x['text'],\n",
    "                    username = x['user']['name'],\n",
    "                    tweet_created_at = x['created_at'],\n",
    "                    query = 'trump',\n",
    "                )]\n",
    "            TWITTERTWEET.objects.bulk_create(tweets)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CommandError(e)\n",
    "        self.stdout.write(self.style.SUCCESS('SuccessQ'))\n",
    "        \n",
    "# VIEW, take the stuff from your database and put it up via html.\n",
    "def tweet(request, query):\n",
    "    return render(request, 'site/tweet.html', {'tweets': TWITTERTWEET.objects.all().filter(query = 'trump')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futures #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use threading to query giphy\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for word in words_to_query:\n",
    "            futures_list.append(executor.submit(queryGiphy, word))\n",
    "        for future in futures_list:\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                print('generated an exception: %s' % (exc))\n",
    "                print future.exception_info\n",
    "            else:\n",
    "                urls.append(data['image_url'])\n",
    "                words_to_insert.append(data['word'])\n",
    "                \n",
    "# Query giphy appropriately\n",
    "def queryGiphy(word):\n",
    "    url = 'http://api.giphy.com/v1/gifs/search?q=%s&api_key=dc6zaTOxFJmzC' % word\n",
    "    stuff = urllib2.urlopen(url).read()\n",
    "    parsed_json = json.loads(stuff)\n",
    "    if len(parsed_json['data']) > 0:\n",
    "        image_url = parsed_json['data'][0]['images']['fixed_height']['url']\n",
    "    else:\n",
    "        image_url = 'https://www.google.com/search?q=no+image+image&rlz=1C5CHFA_enUS703US703&tbm=isch&imgil=9e9JeDQI0DBZrM%253A%253B029W-ajBtZqZzM%253Bhttps%25253A%25252F%25252Fen.wikipedia.org%25252Fwiki%25252FFile%25253ANo_image_available.svg&source=iu&pf=m&fir=9e9JeDQI0DBZrM%253A%252C029W-ajBtZqZzM%252C_&usg=__wfQjvmn6OcisW89PcyaD_t2gzWY%3D&biw=1345&bih=737&ved=0ahUKEwjsgMWZyNHQAhUWS2MKHS8TBdcQyjcIOA&ei=Y1c_WKzDG5aWjQOvppS4DQ#imgrc=lTV1OfjVLlQ3jM%3A'\n",
    "    return {'image_url':image_url, 'word':word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other #\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Sci-Kit Learn (IRIS)\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE THE DATA (Pandas, Numpy, Python) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINE THE DATA (Python, Numpy, Pandas) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LISTS AND STRINGS MANIPULATION\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extend, append, and insert lists\n",
    "list1 = ['one','two','three']\n",
    "list2 = ['four','five','six']\n",
    "list13 = [1,2,3,4]\n",
    "list13.append([5,6,7])\n",
    "\n",
    "list3 = list1 + list2\n",
    "list5 = range(0,11)\n",
    "list5.extend(range(11,21))\n",
    "\n",
    "list11 = list('abcd')\n",
    "list12 = list11.append(['e','f','g','h'])\n",
    "\n",
    "list6 = list('acde')\n",
    "list6.insert(1,'b')\n",
    "\n",
    "# Counting, Summing, Length, etc\n",
    "list18 = ['green','red','red','green','green','red','purple',1,5,6]\n",
    "count_green = list18.count('green')\n",
    "length = len(list18)\n",
    "length_unique = len(set(list18))\n",
    "summy = sum(filter(lambda i: isinstance(i, int), list18))  # Sums only numbers\n",
    "\n",
    "# String changes\n",
    "string8 = 'MaTt'\n",
    "string8.upper()\n",
    "string8.lower()\n",
    "string8.title()\n",
    "string9 = string8[::-1]\n",
    "\n",
    "# Join strings\n",
    "string1 = 'abcd'\n",
    "string2 = 'efgh'\n",
    "string3 = string1.join(string2)\n",
    "string4 = string1 + string2\n",
    "string10 = list('abcd') * 2\n",
    "\n",
    "# Split and strip strings into lists\n",
    "string5 = 'Hey,dude,hows,it,going'\n",
    "list7 = string5.split(\",\")\n",
    "string6 = ' Michael Jordan '\n",
    "string7 = string6.strip()\n",
    "\n",
    "# Removing and deleting items in list\n",
    "\n",
    "list8 = ['Michael','Matt','John','Matt']\n",
    "list17 = [1,2,3,'a','b','c','d',3,'f']\n",
    "# Del Michael\n",
    "del list8[0]\n",
    "del list17[0:3:1]\n",
    "list17[4] = 'e'\n",
    "# Remove\n",
    "list8.remove('Matt')\n",
    "# Pop\n",
    "list9 = ['Michael','Matt','John','Matt','Ladhoff']\n",
    "list10 = list9.pop(2)\n",
    "\n",
    "\n",
    "# Slicing\n",
    "list14 = [1,3,2,6,5,4,7,8,9,10]\n",
    "list14_sorted = list14[:] # Makes a copy without changing original (sorting)\n",
    "list14_sorted.sort(reverse=True)\n",
    "list15 = list14[0:3:1] # Start:Stop:Step\n",
    "list16 = list14[7-1]\n",
    "\n",
    "\n",
    "# Filter & Lambda\n",
    "list18 = ['soup','dog','salad','cat','great','dog']\n",
    "list19 = list(filter(lambda word: word[0]=='s',list18)) # Returns list with words that start with s\n",
    "\n",
    "# MAP & Lambda\n",
    "list20 = [1,2,3,4,5]\n",
    "list21 = list(map(lambda num: num*3,list20))  # Multiplies every number in list by 3\n",
    "sq = lambda num: num**2\n",
    "sq_5 = sq(5)\n",
    "# Apply\n",
    "\n",
    "\n",
    "# Zip\n",
    "list22 = ['a','b','c','d']\n",
    "list23 = [1,2,3,4]\n",
    "zip_list24 = zip(list22,list23)\n",
    "\n",
    "# In\n",
    "bool1 = 'soup' in list18\n",
    "bool2 = 'soup' not in list18\n",
    "\n",
    "# Enumerate\n",
    "#for step in enumerate(list14):\n",
    "    #print(\"Index: {} Value: {}\").format(*step) # One star does tuples or lists, two stars does dicty.\n",
    "    \n",
    "# Get Unique Values (Set)\n",
    "mylist = ['nowplaying', 'PBS', 'PBS', 'nowplaying', 'job', 'debate', 'thenandnow']\n",
    "mylist_num = [1,2,3,1,2,4,5,6,6,7]\n",
    "myset = set(mylist)\n",
    "np_unique = np.unique(mylist_num)\n",
    "\n",
    "# Return Index #\n",
    "index_two = list1.index(\"two\")\n",
    "\n",
    "# Flatten List of Lists\n",
    "l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n",
    "flattened =[item for sublist in l for item in sublist]\n",
    "\n",
    "# Sentence .format()\n",
    "open1 = 9\n",
    "close1 = 10\n",
    "hours = \"We're open from {open2} to {close2}\".format(open2=open1, close2=close1)     \n",
    "\n",
    "# Other\n",
    "import time\n",
    "def sleepy(seconds_delay):\n",
    "    for i in range(101):\n",
    "        print '\\r'+str(i)+'% completed',\n",
    "        time.sleep(seconds_delay)  # 1 = 1 second\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "\n",
    "\n",
    "list14_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi my name is Matt and my age is 25'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DICTIONARIES\n",
    "dicty = {'Name': 'Matt', 'Age': 25, 'High_School_Years': list(range(2004,2009)),\n",
    "         'face' : {'hair_color': 'brown', 'eye_color': 'blue'}}\n",
    "\n",
    "# Add key/value pairs\n",
    "dicty['new'] = 5\n",
    "dicty.update({'College_Years': list(range(2012,2017)),'Favorite_Sport': 'Basketball'})\n",
    "\n",
    "# Delete keys\n",
    "del dicty['face']\n",
    "\n",
    "# Unpacking keys, values\n",
    "my_string = \"Hi my name is {Name} and my age is {Age}\"\n",
    "my_string.format(**dicty)\n",
    "# Return Keys, Values, and Items (tuple format)\n",
    "keys = dicty.keys()\n",
    "values = dicty.values()\n",
    "items = dicty.items()\n",
    "\n",
    "# Looping\n",
    "dicty2 = {'Name': 'John', 'Age': 15, 'Hair_Color': 'Brown', 'High_School_Years': list(range(2004,2009))}\n",
    "\n",
    "for key, value in dicty2.items():\n",
    "    printy = \"{}: {}\".format(key.title(),value)\n",
    "\n",
    "# Set\n",
    "setty = {1,1,1,2,2,2,3,3,3}\n",
    "setty.add(5)\n",
    "setty.add('cool')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "my_string.format(**dicty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TUPLES\n",
    "\n",
    "tuple1 = tuple([3,6,1,7,10])\n",
    "tuple2 = [(1,2),(3,4),(5,6)]\n",
    "\n",
    "# Packing and Unpacking\n",
    "tuple3 = ('a','b','c')\n",
    "a,b,c = tuple3\n",
    "tuple4 = a,b\n",
    "\n",
    "listy1 = [a for (a,b) in tuple2]\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "\n",
    "listy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 11, 28, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATETIME\n",
    "from datetime import datetime\n",
    "directory = (dir(datetime))\n",
    "\n",
    "# Dates\n",
    "now = datetime.now()\n",
    "today = datetime.today()\n",
    "date = datetime(2016, 11, 28)\n",
    "date_2 = now.date() # Same thing as date, doesn't return seconds or hours.\n",
    "time = now.time()\n",
    "weekday = now.weekday()\n",
    "\n",
    "# Replace\n",
    "now_replaced = now.replace(hour=9, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Add or subtract\n",
    "delta = now - now_replaced # Returns (days, seconds, microseconds)\n",
    "delta_days = delta.days\n",
    "\n",
    "# Timedelta\n",
    "from datetime import timedelta\n",
    "time_delta = now + timedelta(days=3, hours=0, minutes=0, seconds=5, microseconds=0) # Adds 3 days.\n",
    "year = timedelta(days=365)\n",
    "total_seconds = year.total_seconds()\n",
    "hour = timedelta(hours=1)\n",
    "workday = hour * 9\n",
    "tomorrow = datetime.now().replace(hour=9, minute=0) + timedelta(days=1)\n",
    "time_off = tomorrow + workday\n",
    "\n",
    "# Strftime (turns datetime into string) & strptime (turns string into date time)\n",
    "# https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n",
    "strftime_date = now.strftime('%m/%d/%y') # Datetime -> String\n",
    "strptime_date = datetime.strptime('2016-11-28','%Y-%m-%d') # String -> Datetime\n",
    "\n",
    "# Combine\n",
    "today_midnight = datetime.combine(today, time) \n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-04-21 09:00:00 EDT-0400'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytz\n",
    "import pytz\n",
    "import datetime\n",
    "pacific = pytz.timezone('US/Pacific')\n",
    "pacific = pytz.timezone('US/Eastern')\n",
    "fmt = '%Y-%m-%d %H:%M:%S %Z%z' #Z gives us the timezone and z gives us the difference with UTC\n",
    "utc = pytz.utc\n",
    "start = pacific.localize(datetime.datetime(2014, 4, 21, 9)) # Same as \"astimezone\" above.\n",
    "\n",
    "all_time_zones = pytz.all_timezones\n",
    "country_time_zones= pytz.country_timezones['us']\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "\n",
    "# Test results here\n",
    "start.strftime(fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': 'kenneth@teamtreehouse.com',\n",
       " 'first': ' Kenneth',\n",
       " 'job': 'Teacher, Treehouse\\t',\n",
       " 'last': 'Love',\n",
       " 'name': 'Love, Kenneth',\n",
       " 'phone': '(555) 555-5555',\n",
       " 'twitter': '@kennethlove'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REGULAR EXPRESSIONS\n",
    "# https://docs.python.org/2/library/re.html\n",
    "# http://www.regexr.com/\n",
    "import re\n",
    "\n",
    "\n",
    "# Regex Syntax\n",
    "'''\n",
    "\\w = any Unicode word character\n",
    "\\s = any whitespace\n",
    "\\d = any number 0-9\n",
    "\\b = word boundaries (beginning/end of word)\n",
    "\\n = new line\n",
    "\\ = escape character (i.e. search for '(' instead of using it)\n",
    "* = occurs at least 0 times i.e. OPTIONAL/KEEP GOING\n",
    "+ = occurs at least once i.e. REQUIRED/STOP\n",
    "? = occurs exactly 0 or 1 times, i.e. OPTIONAL\n",
    "^ = beggining of string\n",
    "$ = end of string\n",
    "{3} = occurs exactly 3 times\n",
    "{0,3} = occurs 0-3 times\n",
    "{3,} = occurs 3 or more times\n",
    "[a-z] = any lowercase letters from a to z\n",
    "[^2] = character is not a 2\n",
    "[aple] = apple (helps remove duplicates)\n",
    "(a-z) = will only return what's in the parenthesis\n",
    "'''\n",
    "\n",
    "# Loading data\n",
    "pointer_names_file = open(\"names.txt\")\n",
    "data = pointer_names_file.read()\n",
    "pointer_names_file.close()\n",
    "\n",
    "# Match (returns where it's beginning of string)\n",
    "match = re.match(pattern=r'Love',string=data)  # The 'r' makes it a \"raw\" string.\n",
    "# Search (returns first instance)\n",
    "search = re.search(pattern=r'Kenneth',string=data)\n",
    "# Find all (returns list of all instances)\n",
    "find_all = re.findall(pattern=r'King',string=data)\n",
    "\n",
    "\n",
    "# Examples of Regex\n",
    "names = re.findall(pattern=r'\\w*,\\s\\w+',string=data)\n",
    "emails = re.findall(r'[-\\w\\d+.]+@[-\\w\\d.]+', data)\n",
    "treehouse = re.findall(r'\\b[trehous]{9}\\b', data, re.IGNORECASE)  #re.IGNORECASE(I) ignores the case\n",
    "phone = re.findall(r'\\(?\\d{3}\\)?-?\\s?\\d{3}-\\d{4}', data)\n",
    "twitter = re.findall(r'@[\\w\\d]+',data)\n",
    "emails_no_gov = re.findall(r'''\n",
    "                    \\b@[-\\w\\d.]*  # Start of word, an @, and then any # of characters\n",
    "                    [^govt\\t]+  # Ignore 1+ instances of the letters g, o, or v and a tab.\n",
    "                    \\b  # End of word\n",
    "                    ''', data, re.VERBOSE|re.IGNORECASE)  # Verbose(X) is because there is multiple lines\n",
    "\n",
    "# Groups (?P<name>)\n",
    "groups = re.search(r'''\n",
    "            ^(?P<name>[-\\w ]*,\\s[-\\w ]+)\\t  # Last (optional) and first names\n",
    "            (?P<email>[-\\w\\d+.]+@[-\\w\\d.]+)\\t  # Email\n",
    "            (?P<phone>\\(?\\d{3}\\)?-?\\s?\\d{3}-\\d{4})?\\t  # Phone (Optional)\n",
    "            (?P<job>[\\w\\s]+,\\s[\\w\\s.]+)\\t? # Job and Company (Tab after is optional)\n",
    "            (?P<twitter>@[\\w\\d]+)?$  # Twitter (Optional)\n",
    "            ''', data, re.X|re.MULTILINE) # Returns list of tuples. MULTILINE(M) = we are using multiple lines\n",
    "\n",
    "dicty_groups = groups.groupdict()\n",
    "\n",
    "string = 'Perotto, Pier Giorgio'\n",
    "\n",
    "names2 = re.match(r'''\n",
    "    ^(?P<last_name>[\\w]*),\\s                # Last name\n",
    "    (?P<first_name>[\\w\\s\\w]*)$           # First name\n",
    "''', string, re.X)\n",
    "\n",
    "first_name = names2.group(\"first_name\")\n",
    "# Compile\n",
    "compile_group = re.compile(r'''\n",
    "            ^(?P<name>(?P<last>[-\\w ]*),(?P<first>\\s[-\\w ]+))\\t  # Last (optional) and first names\n",
    "            (?P<email>[-\\w\\d+.]+@[-\\w\\d.]+)\\t  # Email\n",
    "            (?P<phone>\\(?\\d{3}\\)?-?\\s?\\d{3}-\\d{4})?\\t  # Phone (Optional)\n",
    "            (?P<job>[\\w\\s]+,\\s[\\w\\s.]+)\\t? # Job and Company (Tab after is optional)\n",
    "            (?P<twitter>@[\\w\\d]+)?$  # Twitter (Optional)\n",
    "            ''', re.X|re.MULTILINE)\n",
    "\n",
    "dicty_group2 = re.search(compile_group, data).groupdict()\n",
    "dicty_group3 = compile_group.search(data).groupdict() # Same as above\n",
    "\n",
    "# Looping\n",
    "listy_names = []\n",
    "printy = []\n",
    "for match in compile_group.finditer(data):\n",
    "    listy_names.append(match.group('name'))\n",
    "    printy.append('{first} {last} <{email}>'.format(**match.groupdict()))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "\n",
    "\n",
    "dicty_group3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASSES\n",
    "\n",
    "class Rectangle:\n",
    "    is_alive = True\n",
    "    \n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "    def area(self):\n",
    "        return self.length * self.width\n",
    "    def change(self, new_var):\n",
    "        self.value = new_var\n",
    "        \n",
    "def test_class():\n",
    "    rectangle_a = Rectangle(length=2,width=5)\n",
    "\n",
    "    print \"Length: \", rectangle_a.length\n",
    "    print \"Width: \", rectangle_a.width\n",
    "    print \"Area: \", rectangle_a.area()\n",
    "\n",
    "    print \"\\n\"\n",
    "\n",
    "    rectangle_b = Rectangle(3,7)\n",
    "\n",
    "    print \"Length: \", rectangle_b.length\n",
    "    print \"Width: \", rectangle_b.width\n",
    "    print \"Area: \", rectangle_b.area()\n",
    "\n",
    "    print rectangle_a.is_alive\n",
    "    rectangle_b.is_alive = False\n",
    "    print rectangle_b.is_alive\n",
    "\n",
    "\n",
    "# Setting Defaults\n",
    "class Rectangle2:\n",
    "#     def __init__(self, length =5, width=10):\n",
    "#         self.length = length\n",
    "#         self.width = width\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.length = kwargs.get('length',5)\n",
    "        self.width = kwargs.get('width',10)\n",
    "\n",
    "rectangle_c = Rectangle2(length=50)\n",
    "rectangle_c.length\n",
    "\n",
    "# Inheritance\n",
    "class Monster(object):\n",
    "    import random\n",
    "    # Default Values that overrides inheritence class.\n",
    "    COLORS = ['yellow','red','blue','green']\n",
    "    min_hit_points = 1\n",
    "    max_hit_points = 1\n",
    "    min_experience = 1\n",
    "    max_experience = 1\n",
    "    weapon = \"Sword\"\n",
    "    sound = \"ROAR\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        self.hit_points = random.randint(self.min_hit_points,self.max_hit_points)\n",
    "        self.experience = random.randint(self.min_experience,self.max_experience)\n",
    "        self.color = random.choice(COLORS)\n",
    "        \n",
    "        for key,value in kwargs.items():\n",
    "            setattr(self,key,value)\n",
    "    \n",
    "    def __str__(self): #Try print(pete) to understand what this does\n",
    "        return \"{} {}, HP: {}, XP: {}\".format(self.color.title(),self.__class__.__name__,self.hit_points,self.experience)\n",
    "    \n",
    "# Subclass #1            \n",
    "class Goblin(Monster):\n",
    "    max_hit_points = 3\n",
    "    max_experience = 2\n",
    "    sound = \"SQUEEK\"\n",
    "# Subclass #2\n",
    "class Dragon(Monster):\n",
    "    max_hit_points = 10\n",
    "    max_experience = 10\n",
    "    sound = \"FIREEE\"\n",
    "\n",
    "azog = Goblin()\n",
    "pete = Dragon()\n",
    "  \n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Test Here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Better Python\n",
    "\n",
    "# PEP 8(syntax/readability),20(principles/zen of python),257(for docstrings)\n",
    "\n",
    "# https://www.python.org/dev/peps/pep-0008/\n",
    "# http://pep8online.com\n",
    "\n",
    "import this #Pep 20 (zen of python)\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.info(\"You won't see this\") # Users won't see this and you can put in anywhere in code.\n",
    "logging.debug(\"You won't see this\") # Users won't see this and you can put in anywhere in code.\n",
    "logging.warn(\"OH NO\") # Users will see this\n",
    "logging.basicConfig(filename='game.log', level=logging.DEBUG) # This is where all your logs or hidden messages will go\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#    \n",
    "# Python Debugger\n",
    "\n",
    "my_list = [5, 2, 1, True, 'abcdefg', 3, False, 4]\n",
    "\n",
    "import pdb; pdb.set_trace() # Make sure to remove this after using\n",
    "del my_list[3]\n",
    "del my_list[4]\n",
    "del my_list[6]\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create Array\n",
    "array1 = np.array([1,2,3])  # One dimension (1x3)\n",
    "array2 = np.array([[1,2,3],[4,5,6],[7,8,9]])  # Three dimensions (3x3)\n",
    "array2_copy = np.array([[10,11,12],[13,14,15],[16,17,18]])\n",
    "array3 = np.arange(0,11)  # Start, stop, step\n",
    "\n",
    "array4 = np.zeros(shape=(3,4))  # Rows, columns\n",
    "array5 = np.ones(shape=(3,4))\n",
    "array6 = np.linspace(0,5,10) # One dimensional array consisting of 10 values between 0-5\n",
    "\n",
    "array7 = np.random.randn(5,5)  # 5x5 Matrix with random normal distributed numbers from 0-1\n",
    "\n",
    "array8 = np.eye(4) # Identitiy matrix\n",
    "\n",
    "# Reshape Array\n",
    "array9 = array4.reshape(2,6)  # Rows, columns\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "\n",
    "# Max, Min, Sum\n",
    "# https://docs.scipy.org/doc/numpy/reference/ufuncs.html <<< Complete list of functions\n",
    "minny = array7.min()\n",
    "maxxy = array7.max()\n",
    "summy_cols = array7.sum(axis=0)  # Get sum of each column (axis=0) or rows (axis=1)\n",
    "maxxy_index = array7.argmax()\n",
    "array10 = array2 + array2_copy  # Add two matrices together\n",
    "array11 = array2 + 100  # Adds 100 to every value in matrix\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Slicing\n",
    "\n",
    "# Changing and copying values\n",
    "array9 = array3.copy()[0:5]  # Start, Stop, Step. # If you don't have copy() array3 will be changed below (try it)\n",
    "array9[:] = 99 # This also changes array3 to have the first 5 numbers = 99.\n",
    "# Slicing a Matrix\n",
    "row1 = array2[0,:]  #Use 1 to get second row, etc\n",
    "column = array2[:,1]\n",
    "value = array2[2,1]  # Row, Column\n",
    "top_right = array2[0:2,1:]\n",
    "# Conditional Slicing\n",
    "greater_than = array2 > 3\n",
    "greater_than_v2 = array2[greater_than]\n",
    "# Replacing (similar to map)\n",
    "array10 = list(np.where(array3 > 3,'yes','no'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Random Generator\n",
    "import random\n",
    "\n",
    "random4 = np.random.randint(0,100,size=(100,10))  # Size is optional\n",
    "random5 = random.sample(xrange(100), 10)  # Random list of 10 numbers\n",
    "random6 = np.random.rand(5)  # Vector with random uniform distributed numbers from 0-1\n",
    "random7 = np.random.randn(5,5)  # 5x5 Matrix with random normal distributed numbers from 0-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#print summy_cols\n",
    "# Test Here\n",
    "\n",
    "array3\n",
    "\n",
    "array10 = list(np.where(array3 > 3,'yes','no'))\n",
    "\n",
    "array10\n",
    "\n",
    "array10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and display DataFrames\n",
    "\n",
    "data1 = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n",
    "        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n",
    "        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n",
    "        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n",
    "labels1 = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "data2 = {'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n",
    "                               'Budapest_PaRis', 'Brussels_londOn'],\n",
    "              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n",
    "              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n",
    "                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )', \n",
    "                               '12. Air France', '\"Swiss Air\"']}\n",
    "\n",
    "data_pivot = {'A':['foo','foo','foo','bar','bar','bar'],\n",
    "     'B':['one','one','two','two','one','one'],\n",
    "       'C':['x','y','x','y','x','y'],\n",
    "       'D':[1,3,2,5,4,1]}\n",
    "\n",
    "# Create\n",
    "df = pd.DataFrame(data1, index=labels1)\n",
    "df2 = pd.DataFrame(data=np.random.randint(0,100,size=(100,10)), columns=list('ABCDEFGHIJ'))\n",
    "df3 = pd.DataFrame(data2)\n",
    "df_pivot = pd.DataFrame(data_pivot)\n",
    "\n",
    "# Display Info\n",
    "info = df.info()\n",
    "describe = df.describe\n",
    "types = df.dtypes\n",
    "shape = df.shape\n",
    "head = df.head()\n",
    "tail = df.tail()\n",
    "columns = list(df.columns.values)\n",
    "df.columns.names = ['Columns']\n",
    "df.index.names = ['Indexes']\n",
    "vc_columnname = df[''].value_counts()\n",
    "# Percent of Total\n",
    "vc_columnname_pot = df.groupby('').size() * 100 / len(df)\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n",
    "\n",
    "df.ix[(df.age >=2) & (df.age <= 4), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series\n",
    "\n",
    "series1 = pd.Series(data=[10,20,30], index=['a','b','c'])\n",
    "series2 = pd.Series(data=[40,50,60], index=['a','b','d'])\n",
    "series3 = pd.Series(data=[sum,min,len])  # You can even put functions in\n",
    "\n",
    "series_summy = series1 + series2  # Matches on index value, creates new index with NaN if there is no match.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Slicing\n",
    "\n",
    "a = df[0:3]  # First 3 rows\n",
    "a_2 = df.ix[['a','b','c']] # Rows that are index a,b,c\n",
    "b = df[['animal', 'age']] # Columns animal and age\n",
    "b_2 = df.ix['b', 'animal'] # Index b from column animal.\n",
    "c = df.ix[[3,4,7], ['animal','age']] # Rows 3,4,7 and columns 'animal' and 'age'\n",
    "d = df.ix[df.visits > 2, :] # All rows/columns with visits > 2\n",
    "e = df.ix[df.age.isnull(), :] # All rows with age = null\n",
    "f = df.ix[(df.age >=2) & (df.age <= 4), :] # Must use & for and. Use | for or.\n",
    "g = df.age['f'] = 1.5 # Change age in row F to 1.5\n",
    "h = sum(df.visits)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Insert and append, delete, and sort Column(s) & Rows\n",
    "\n",
    "# Adding a column/row\n",
    "df['sum_age_visit'] = df['age'] + df['visit']  # Column at the end\n",
    "df.insert(1,column='New_Column',value=random.sample(xrange(100), 10)) # Column at specific column spot\n",
    "l = df.append(df[0:3],ignore_index=True)  # Row\n",
    "\n",
    "# Remove a column/row\n",
    "df = df.drop('New_Column',axis=1,inplace=True)  # Column\n",
    "#del df['k']\n",
    "df.drop(['a','b'],axis=0,inplace=True)  # Row\n",
    "\n",
    "# Sorting\n",
    "i = df.sort(by=['age','visits'], ascending=[False,True]) #Sort by age largest to smallest, then visits, smallest to largest\n",
    "\n",
    "# Changing type of column\n",
    "df.age = df.age.astype(str)\n",
    "\n",
    "# Get and label columns\n",
    "columns = list(df.columns.values)\n",
    "df.columns.names = ['columnsy']\n",
    "\n",
    "# Label index\n",
    "df.index.names = ['indexy']\n",
    "# Change index name\n",
    "m = df.rename(index={1: 'a'})\n",
    "# Reset/Set index\n",
    "df.set_index('animal')\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Operations (Apply, Groupby, Pivot Tables, Value Count, Unique Values, Melting, Dummies, REGEX, etc)\n",
    "\n",
    "# Apply (applies a custom funciton to each element in column)\n",
    "df_apply = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']})\n",
    "\n",
    "apply_lambda = df_apply['col1'].apply(lambda x: x**2)  # Squares every number in col1 (can also use def function)\n",
    "apply_len = df_apply['col3'].apply(len)  # Returns length of each string in column3\n",
    "\n",
    "# Replace, map, and where\n",
    "j = df['animal'].replace('snake','python')\n",
    "k = df['priority'].map({'yes': 1,'no': 0})\n",
    "df['new_animal'] = np.where(df['animal']=='snake', 'python', 'not_snake')\n",
    "\n",
    "\n",
    "# Groupby\n",
    "i = df.groupby('animal')['age'].mean()\n",
    "groupby_describe = df.groupby('animal')['age'].describe()\n",
    "\n",
    "# Pivot Table\n",
    "pivot = pd.pivot_table(data=df, values='age', index='animal', columns='visits', aggfunc='sum', fill_value='No_Age')\n",
    "pivot1 = pivot[2] #Showing that pivot tables are like dataframes\n",
    "\n",
    "pivot2 = df_pivot.pivot_table(values='D',index=['A', 'B'],columns=['C'])\n",
    "\n",
    "# Text to columns (split and create new columns)\n",
    "temp = df3['From_To'].str.split('_', expand=True)\n",
    "temp.columns = ['From', 'To']\n",
    "temp['From'] = temp['From'].str.capitalize()\n",
    "temp['To'] = temp['To'].str.capitalize()\n",
    "\n",
    "temp = df.From_To.str.split('_', expand=True)\n",
    "temp.columns = ['From', 'To']\n",
    "\n",
    "# Value Count\n",
    "h = df.animal.value_counts()\n",
    "# Returns # of unique values\n",
    "nunique = df.animal.nunique()\n",
    "# Returns unique values\n",
    "unique = df.animal.unique()\n",
    "\n",
    "# Using REGEX filters\n",
    "import re\n",
    "df3['Airline_Cleaned'] = df3['Airline'].str.extract('([a-zA-Z\\s]+)', expand=False).str.strip()\n",
    "airline = re.findall('([a-zA-Z\\s]+)',df3['Airline'][0])\n",
    "\n",
    "# Create dummy variables (dummies)\n",
    "dummy = pd.get_dummies(df['priority'],drop_first=True)  # Drop first means to drop the first column of dummies.\n",
    "dummy['Id_Vars'] = random.sample(xrange(100), 10) # Random list of 10 numbers\n",
    "\n",
    "# Melting\n",
    "melt = pd.melt(frame=dummy, id_vars=['Id_Vars'],var_name='Animal',value_name='Value')\n",
    "\n",
    "# Drop Duplicates\n",
    "dd = df.drop_duplicates(keep=False)\n",
    "# Unstack into single series\n",
    "unstack = df.unstack()\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "\n",
    "# Multindex Dataframes\n",
    "outside = ['G1','G1','G1','G2','G2','G2']\n",
    "inside = [1,2,3,1,2,3]\n",
    "hier_index = list(zip(outside,inside))\n",
    "hier_index = pd.MultiIndex.from_tuples(hier_index)\n",
    "df_mi = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B'])\n",
    "\n",
    "df_mi.index.names=['Outside','Inside']  # Renames each level in the index\n",
    "\n",
    "# Slicing Multinex\n",
    "mi_slice = df_mi.ix['G1'].ix[1]  # Normal slicing\n",
    "mi_xs = df_mi.xs(1,level='Inside')  # Returns all values of inside index = 1\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Dealing with Nulls/Mising data\n",
    "\n",
    "df_null = {'A':[1,2,np.nan], 'B':[5,np.nan,np.nan], 'C':[1,2,3]}\n",
    "\n",
    "# Find nulls\n",
    "df_null.isnull()\n",
    "# Replace nulls test\n",
    "df_null.fillna('Filled',inplace=True,axis=1)\n",
    "# Drop nulls\n",
    "df_null.dropna(axis=0,thresh=2)  # Drops all rows/columns with null with null values >=2, thres is OPTIONAL\n",
    "# Heatmap of null values (yellow = null)\n",
    "sns.set_style('whitegrid')\n",
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Merging, Joining, and Concanating dataframes\n",
    "\n",
    "# Concatination (adding DF's together)\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                        'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                        'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                        'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                        index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                        'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                        'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                        'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                         index=[4, 5, 6, 7]) \n",
    "\n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                        'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                        'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                        'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                        index=[8, 9, 10, 11])\n",
    "\n",
    "df_concat1 = pd.concat([df1,df2,df3])  # By default it adds rows to bottom of eachother\n",
    "df_concat2 = pd.concat([df1,df2,df3], axis=1)  # When you join on columns here, since some match, the indexes must match too\n",
    "df_concat3 = pd.concat([df1,df2,df3], axis=0) # This workds because none of our indexes match and it's just adding together\n",
    "\n",
    "# Merging (similar to v-lookup/merging SQL tables based on unique value in column)\n",
    "left = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'key2': ['K0', 'K0', 'K1', 'K2'],\n",
    "                     'key3': ['K0', 'K1', 'K0', 'K1'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "    \n",
    "right = pd.DataFrame({'key1': ['K0', 'K1', 'K2', 'K3'],\n",
    "                      'key2': ['K0', 'K1', 'K1', 'K2'],\n",
    "                      'key3': ['K0', 'K0', 'K0', 'K0'],\n",
    "                      'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                      'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "\n",
    "df_merge1 = pd.merge(left,right,how='inner',left_on='key1',right_on='key1')  # Merging on single unique ID\n",
    "df_merge2 = pd.merge(left,right,how='inner',on=['key1','key2']) # Merging on combination of two unique IDs\n",
    "\n",
    "# Joining (uses index)\n",
    "left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n",
    "                     'B': ['B0', 'B1', 'B2']},\n",
    "                      index=['K0', 'K1', 'K2']) \n",
    "\n",
    "right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D2', 'D3']},\n",
    "                      index=['K0', 'K2', 'K3'])\n",
    "\n",
    "join_inner = left.join(right,how='inner')\n",
    "join_outer = left.join(right,how='outer')\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Date and time\n",
    "\n",
    "p = pd.date_range(start='2015-01-01', end='2015-12-31', freq='B') \n",
    "df['_started_at'] =  pd.to_datetime(df['_started_at'], format = \"%m-%d-%Y\")\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Data Input and Output\n",
    "\n",
    "# tip = type pwd to get location of current file\n",
    "\n",
    "# CSV\n",
    "df_csv = pd.read_csv('example')\n",
    "df_csv.to_csv('My_output', index=False)  # Index=False keeps your same indexes, otherwise it will use index as first column\n",
    "\n",
    "# Excel\n",
    "df_excel = pd.read_excel('Excel_Sample.xlsx', sheetname='Sheet1')\n",
    "df_excel.to_excel('Excel_Sample2.xlsx',sheet_name='NewSheet')\n",
    "\n",
    "# HTML\n",
    "listy_HTML = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')  # By default creates a list of tables\n",
    "df_HTML = listy_HTML[0]\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "engine_sqlite = create_engine('sqlite:///:memory:')\n",
    "engine_postgres = create_engine('postgres://mfnyeqfz:zxzi25vFYOiloQ8ORKUo2-Oog5GgfZc2@elmer.db.elephantsql.com:5432/mfnyeqfz')\n",
    "\n",
    "df_csv.to_sql('my_table', engine_sqlite)\n",
    "df_sql = pd.read_sql('my_table', con=engine_sqlite)\n",
    "select = pd.read_sql(\"SELECT a,b FROM my_table WHERE d > 7\", con=engine_sqlite)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# ADVANCED\n",
    "\n",
    "df2 = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7],'B':list('abcefghijkl')})\n",
    "\n",
    "# Shift data down (negative) or up (positive) a row (axis=0) or column (axis=1)\n",
    "df3 = df2.shift(periods=-1,axis=0)\n",
    "# Get mean,sum, min, max of every row(index) or columns\n",
    "mean = df.mean(axis='index')\n",
    "summy = df.sum(axis='index',numeric_only=True)\n",
    "# Subtract columns/rows from specified value\n",
    "df.sub(df.mean(axis=1), axis=0)\n",
    "# Find columns that is the min or max\n",
    "min_col = summy.idxmin(axis='columns')\n",
    "# Find index that is min or max\n",
    "min_index = df.age.idxmin(axis='index')\n",
    "# Return the biggest X number of values from column\n",
    "nlargest = df.age.nlargest(2)\n",
    "df.animal.argmax\n",
    "# Percent of column that is null\n",
    "df.isnull().apply(lambda x: x/len(df), axis=0).sum()\n",
    "# Drop columns with a lot of nan's\n",
    "df_null.dropna(axis=1,thresh=(.90*len(df))\n",
    "# To_Numeric\n",
    "fastfood_df['gramsfat'] = pd.to_numeric(fastfood_df['gramsfat'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn (Count Vectorizer, Label Encoder, OneHotEncoder, Matrix Creation, Pipelines, etc) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Label Encoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([1, 2, 2, 6])\n",
    "\n",
    "le_classes = (le.classes_)\n",
    "le_transform = le.transform([1, 2, 1, 6]) \n",
    "\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\", \"cool\"])\n",
    "\n",
    "le_classes_2 = (le.classes_)\n",
    "le_transform = le.transform([\"tokyo\", \"tokyo\", \"paris\", \"amsterdam\", \"cool\"]) \n",
    "le_inverse_transform = le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Vectorizer of Words\n",
    "\n",
    "# Remove punctioation and stop words from messages and create list of words for each message\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "def custom_filter(message):\n",
    "    nopunc = [char for char in message if char not in string.punctuation]\n",
    "    nopunc = \"\".join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "df_text_messages['message'] = df_text_messages['message'].apply(custome_filter)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# (PIPELINE REPLACES THIS) Create Vector for Each Message (2x2 Sparse Matrix of Word Counts where columns = messages, rows word 1 count, word 2 count, etc)\n",
    "                       #This Image Helps Understand = <img src=\"Count_Vectorizer.png\">#\n",
    "\n",
    "# Count Vectorizer/Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_count = CountVectorizer(analyzer=custom_filter).fit_transform(X_train)\n",
    "\n",
    "# Create TfidF Vectorizer (Returns weight value instead of count of word)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer_tfidf = TfidfTransformer().fit_transform(df_text_messages_bow)\n",
    "\n",
    "# Visualize the vectorizer (OPTIONAL) (Rows are newsgroups, columns are earch word)\n",
    "df_vectorizer_data = vectorizer_tfidf.transform(X_train).todense()\n",
    "df_vectorizer = pd.DataFrame(df_vectorizer_data, columns = list(vectorizer_tfidf.vocabulary_))\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Good Info (OPTIONAL)\n",
    "\n",
    "# Get most important words using K-Best\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "columns = pd.Series(vectorizer_tfidf.get_feature_names())\n",
    "selector = SelectKBest(f_classif, k=50)\n",
    "selected_data = selector.fit_transform(X_train_,y_train)\n",
    "top_words = columns[selector.get_support()]\n",
    "\n",
    "# Get Importance Value for University\n",
    "# tfidf_frequency_university = tfidf_transformer.idf_[bow_transformer.vocabulary_['university']]\n",
    "unique_words = bow_transformer.vocabulary_\n",
    "sparsity_measure = sparsity = (100.0 * df_text_messages_bow.nnz / (df_text_messages_bow.shape[0] * df_text_messages_bow.shape[1]))  # Non-Zero Messages/Total Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Pipeline (Performs/Replaces The Above Steps)\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('bow',CountVectorizer(analyzer=custom_filter)),  # Turns each message into 2x2 Sparse Matrix where columns = messages, rows word 1 count, word 2 count, etc)\n",
    "        ('tfidf',TfidfTransformer()),  # Turns bag of word counts into weight values\n",
    "        ('classifier',MultinomialNB())  # Uses weight values to create/fit model\n",
    "        ])\n",
    "\n",
    "pipeline.fit(X_train,y_train)  # Pass in text data and it runs the pipeline on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = pipeline.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create matrix with user_id x Title\n",
    "matrix_movie = df.pivot_table(index='user_id',columns='title',values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL (MySQL, PostgreSQL) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine The Data & Find Patterns (Matplotlib, Seaborn, Plotly, Tableau, etc ) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# www.matplotlib.org\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(0,5,11)\n",
    "y = x ** 2\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All In One\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_axes([0,0,1,1]) # Left, Bottom, Width, Height. Creates slope of line essentially.\n",
    "ax.plot(x,y,label='X,Y',color='green',lw=.5,marker=\"+\",markersize=3)  # Assign data to plot and set label\n",
    "ax.plot(y,x,label='Y,X',color='red',lw=3,ls=':')\n",
    "\n",
    "ax.set_title('Title')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_xlabel('X')\n",
    "ax.legend(loc=0)  # Puts the legend on the graph. 0 = Best Location. Visit http://matplotlib.org/api/legend_api.html for all legend types.\n",
    "\n",
    "ax.set_xlim([0,2]) # Will only show graph for X = 0 to 2\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Basic Scatter Plot\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('X Label')\n",
    "plt.ylabel('Y Label')\n",
    "plt.title('Title')\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Plot multiple graphs (subplot)\n",
    "\n",
    "# Simple Way\n",
    "plt.subplot(1,2,1)  # Rows, Columns, Plot unique #\n",
    "plt.plot(x,y,'r')  # 'r' = red\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y,x,'b')\n",
    "\n",
    "# Advanced Way\n",
    "fig,axes = plt.subplots(nrows=1,ncols=2)\n",
    "axes[0].plot(x,y)\n",
    "axes[0].set_title('First Plot')\n",
    "\n",
    "axes[1].plot(x,y)\n",
    "axes[1].set_title('Second Plot')\n",
    "plt.tight_layout()  # Removes overlaps of multiple graphs\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Figure Size and DPI\n",
    "fig,axes = plt.subplots(nrows=2, ncols=1,figsize(8,2))\n",
    "\n",
    "axes[0].plot(x,y)\n",
    "axes[1].plot(y,x)\n",
    "plt.tight_layout()\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Save a Figure\n",
    "fig.savefig('my_picture.png',dpi=200)  # DPI is optional, sets detail of map.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Object Oriented (plots within plots)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "axes1 = fig1.add_axes([0.1,0.1,.8,0.8]) # Left, Bottom, Width, Height. Creates slope of line essentially.\n",
    "axes2 = fig1.add_axes([0.2,0.5,0.4,0.3])\n",
    "axes1.plot(x,y)\n",
    "axes2.plot(y,x)\n",
    "\n",
    "axes1.set_title('LARGER')\n",
    "axes2.set_title('SMALLER')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seaborn #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x115e75dd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFoCAYAAABkAJMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1159677d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seaborn\n",
    "# EXAMPLES OF GRAPHS: http://seaborn.pydata.org/examples/index.html\n",
    "# LIST OF FUNCTIONS: http://seaborn.pydata.org/api.html\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "tips.head()\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n",
    "\n",
    "# Matrix Data\n",
    "flights = sns.load_dataset('flights')\n",
    "\n",
    "# Heatmap\n",
    "tc = tips.corr()  # Creates lables for index (heatmap must have that)\n",
    "sns.heatmap(tc,annot=True,cmap='coolwarm')  # annot shows the actual value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Numerical Continous Data\n",
    "\n",
    "# Distribution Plot\n",
    "sns.distplot(tips['total_bill'],kde=True,bins=40)  # KDE (sums of the normal distributions at each point) and Bins are optional\n",
    "# Joint Plot (distribution plot for multiple variables)\n",
    "sns.jointplot(x='total_bill',y='tip',data=tips,kind ='reg')  # Kind are optional (can use hex or kde)\n",
    "# Pair Plot (does a joint plot for every combination of variables in a datafram)\n",
    "sns.pairplot(tips,hue='sex',palette='coolwarm')  # Hue is a different color for each value in a categorical value\n",
    "# Rug Plot (like histogram but has a line for each data point instead of a bar)\n",
    "sns.rugplot(tips['total_bill'])\n",
    "# KDE Plot\n",
    "sns.kdeplot(tips['total_bill'])\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Categorical Data\n",
    "\n",
    "# Factor Plot (can replace kind with any plots below)\n",
    "sns.factorplot(x='day',y='total_bill',data=tips,kind='bar') #\n",
    "# Bar Plot \n",
    "sns.barplot(x='sex',y='total_bill',data=tips,estimator=np.mean)  # Can use any function in the estimator\n",
    "# Count Plot\n",
    "sns.countplot(x='sex',data=tips)\n",
    "# Box & Whisker Plot (Simplier than Violin Plot)\n",
    "sns.boxplot(x='day',y='total_bill',data=tips,hue='smoker')  # Hue is Optional\n",
    "# Violin Plot (Best One)\n",
    "sns.violinplot(x='day',y='total_bill',data=tips,hue='smoker',split=True)  # The width is the distribution (wider=more points)\n",
    "\n",
    "# Strip Plot (Scatterplot based on categorical variable)\n",
    "sns.stripplot(x='day',y='total_bill',data=tips,jitter=True)\n",
    "# Swarm Plot (Strip + Violin Plot)\n",
    "sns.swarmplot(x='day',y='total_bill',data=tips)  # DON\"T USE ON BIG DATA\n",
    "# Combination of Swarm and Violin Plot\n",
    "sns.violinplot(x='day',y='total_bill',data=tips,hue='smoker',split=True)  # The width is the distribution (wider=more points)\n",
    "sns.swarmplot(x='day',y='total_bill',data=tips,color='black')  # DON\"T USE ON BIG DATA\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Matrix Data\n",
    "flights = sns.load_dataset('flights')\n",
    "\n",
    "# Heatmap\n",
    "tc = tips.corr()  # Creates lables for index (heatmap must have that)\n",
    "sns.heatmap(tc,annot=True,cmap='coolwarm')  # annot shows the actual value\n",
    "\n",
    "fp = flights.pivot(index='month',columns='year',values='passengers')  # Creates labeled index/matrix\n",
    "sns.heatmap(fp,cmap='coolwarm',linecolor='white',linewidths=1)\n",
    "\n",
    "# Cluster Map\n",
    "sns.clustermap(fp,cmap='coolwarm',standard_scale=1)  # Groups columns and rows that are similar to eachother together.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Regression Plot\n",
    "\n",
    "# LM Plot (Scatter with Regression Line)\n",
    "sns.lmplot(x='total_bill',y='tip',data=tips,hue='day',markers=['o','v'],scatter_kws={'s':50})  # Check documentation for more scatter_kws options\n",
    "sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',col='day',aspect=0.6,size=8) # Col (or row) splits into multiple graphs\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Grids\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "# Pair Grid (Can assign type/location of plot to grid)\n",
    "pg = sns.PairGrid(iris)\n",
    "pg.map(plt.scatter)  # Assign type of plot to the grid\n",
    "pg.map_diag(sns.distplot)  # Assign to section of grid\n",
    "pg.map_upper(plt.scatter)\n",
    "pg.map_lower(sns.kdeplot)\n",
    "\n",
    "# Facet Grid (allows you to seperate by categorical value and have custom distribution plot on seperate variable)\n",
    "fg = sns.FacetGrid(data=tips,col='time',row='smoker')\n",
    "fg.map(sns.distplot,'total_bill')\n",
    "fg.map(plt.scatter,'total_bill','tip')  # Showing how to do more than one argument function (scatter)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Style and Color\n",
    "\n",
    "# Main Color/Pallete Maps: http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "# Creates blank white grid background for all plots. Other option is ticks\n",
    "sns.set_style('whitegrid')  \n",
    "# Removes right and top spines (try with style='ticks')\n",
    "sns.despine(left=True)\n",
    "# Sets size of figure\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "# Notebook for iPython. Poster for printing on poster.\n",
    "sns.set_context(context='poster')\n",
    "\n",
    "sns.countplot(x='sex',data=tips)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Misc\n",
    "\n",
    "# Move Legend\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly and Cufflinks (Interactive) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df1 = pd.read_csv('df1',index_col=0)\n",
    "df2 = pd.read_csv('df2')\n",
    "\n",
    "# Histogram\n",
    "df1['A'].plot.hist(bins=30)\n",
    "# Histogram on two columns\n",
    "df_text_messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names=['label','message'])\n",
    "messages.hist(column='length', by='label', bins=50,figsize=(12,4))\n",
    "# Line Plot\n",
    "df1.plot.line(x=df1.index,y='B',figsize=(12,3),lw=1)  # Can use any matplotlib function\n",
    "# Scatter Plot\n",
    "df1.plot.scatter(x='A',y='B',c='C',s=df1['C']*100)  # c=color, s=size\n",
    "# Bar Plot\n",
    "df2.plot.bar(stacked=True)\n",
    "# Box Plot\n",
    "df2.plot.box()\n",
    "# Area Plot\n",
    "df2.plot.area(alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Model (Scikit-Learn, Supervised, Unsupervised, Re-inforcement) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ML_Process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bias_Variance_Tradeoff.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bias_Variance_Tradeoff_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Sci_Kit_Learn_Cheat_Sheet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Model_Info.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Framework #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Load in Data\n",
    "df = pd.read_csv('USA_Housing.csv')\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Clean The Data\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Feature Engineering (OPTIONAL)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Assign X,y variable\n",
    "X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "               'Avg. Area Number of Bedrooms', 'Area Population']]  # Features/Input Data\n",
    "y = df['Price']  # Variable your trying to predict/Output\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Scale the features (X) (OPTIONAL)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = df_X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Split into Training and Testing Set (OPTIONAL)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_lm = LinearRegression()\n",
    "model_lm.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_lm.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "# Look at parts of model (coeficient, intercept, etc) (OPTIONAL)\n",
    "df_coef = pd.DataFrame(model_lm.coef_[0],index=list(X.columns),columns=['Coefficient'])  # 1 unit increase in [index] increases price by [Coef Value]\n",
    "intercept = model_lm.intercept_\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Pipeline (OPTIONAL)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('bow',CountVectorizer(analyzer=custom_filter)),  # Turns each message into 2x2 Sparse Matrix where columns = messages, rows word 1 count, word 2 count, etc)\n",
    "        ('tfidf',TfidfTransformer()),  # Turns bag of word counts into weight values\n",
    "        ('model_classifier',MultinomialNB())  # Uses weight values to create/fit model\n",
    "        ])\n",
    "\n",
    "pipeline.fit(X_train,y_train)  # Pass in text data and it runs the pipeline on it\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_lm.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Visualize Predictions (OPTIONAL)\n",
    "plt.scatter(y_test, predictions_values,cmap='coolwarm')\n",
    "sns.distplot((y_test-predictions_values)) # Check to make sure if difference between predictions and actuals are evenly distributed (more should be near 0)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report,mean_squared_error,r2_score\n",
    "sqrt_mean_squared_error = np.sqrt(sklearn.metrics.mean_squared_error(y_test,predictions_values))\n",
    "predictions_score = sklearn.metrics.r2_score(y_test,predictions_values)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Regression_Shaq_Example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Linear_Regression_Goal.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917682400965\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Avg. Area Income</th>\n",
       "      <td>21.528276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg. Area House Age</th>\n",
       "      <td>164883.282027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg. Area Number of Rooms</th>\n",
       "      <td>122368.678027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg. Area Number of Bedrooms</th>\n",
       "      <td>2233.801864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area Population</th>\n",
       "      <td>15.150420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coefficient\n",
       "Avg. Area Income                  21.528276\n",
       "Avg. Area House Age           164883.282027\n",
       "Avg. Area Number of Rooms     122368.678027\n",
       "Avg. Area Number of Bedrooms    2233.801864\n",
       "Area Population                   15.150420"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Linear Regression\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load in Data\n",
    "df = pd.read_csv('USA_Housing.csv')\n",
    "\n",
    "# Assign X,y variable\n",
    "X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "               'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "y = df['Price']  # Variable your trying to predict\n",
    "\n",
    "# Split into Training and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_lm = LinearRegression()\n",
    "model_lm.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_lm.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "# Look at parts of model (coeficient, intercept, etc) (OPTIONAL)\n",
    "df_coef = pd.DataFrame(model_lm.coef_,X.columns,columns=['Coefficient'])  # 1 unit increase in [index] increases price by [Coef Value]\n",
    "intercept = model_lm.intercept_\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_lm.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Visualize Predictions (OPTIONAL)\n",
    "plt.scatter(y_test, predictions_values,cmap='coolwarm')\n",
    "sns.distplot((y_test-predictions_values)) # Check to make sure if difference between predictions and actuals are evenly distributed (more should be near 0)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "import sklearn.metrics\n",
    "sqrt_mean_squared_error = np.sqrt(sklearn.metrics.mean_squared_error(y_test,predictions_values))\n",
    "predictions_score = sklearn.metrics.r2_score(y_test,predictions_values)\n",
    "\n",
    "print(predictions_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n",
    "\n",
    "\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Sigmoid_Function.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.784955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.025864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>-0.085293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.004686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>-2.321072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>-0.015281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>-0.169774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Coefficient\n",
       "Pclass    -0.784955\n",
       "Age       -0.025864\n",
       "SibSp     -0.216500\n",
       "Parch     -0.085293\n",
       "Fare       0.004686\n",
       "male      -2.321072\n",
       "Q         -0.015281\n",
       "S         -0.169774"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Logistic Regression\n",
    "\n",
    "# Load The Data\n",
    "df_train_cleaned = pd.read_csv('titanic_train_cleaned')\n",
    "\n",
    "# Assign X,y variable (must be numeric data only)\n",
    "X = df_train_cleaned.drop('Survived',axis=1)  # Features/Input Data\n",
    "y = df_train_cleaned['Survived']  # Variable your trying to predict/Output\n",
    "\n",
    "# Split into Training and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_log.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "# Look at parts of model (coeficient, intercept, etc) (OPTIONAL)\n",
    "df_coef = pd.DataFrame(model_log.coef_[0],index=list(X.columns),columns=['Coefficient'])  # 1 unit increase in [index] increases price by [Coef Value]\n",
    "intercept = model_log.intercept_\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_log.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors (KNN) - Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"KNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"KNN_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# KNN - Classification\n",
    "\n",
    "# Load The Data\n",
    "df = pd.read_csv('Classified Data',index_col=0)  # Annonymous data\n",
    "\n",
    "# Assign X,y variable\n",
    "X = df.drop('TARGET CLASS',axis=1)\n",
    "y = df['TARGET CLASS']  # Variable your trying to predict/Output\n",
    " \n",
    "# Scale The Features (X)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.tbransform(X)\n",
    "X = df_X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "# Split into Training and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier(n_neighbors=17)  # Used elbow method below to determine n_neighbors (k value)\n",
    "model_knn.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_knn.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_knn.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Elbow Method To Determine Best K (Look at graph, 17 is best, lowest least bumpy)\n",
    "\n",
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    predictions_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(predictions_i != y_test))\n",
    "    \n",
    "\n",
    "plt.Figure(figsize=(20,30))\n",
    "plt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',\n",
    "                     markerfacecolor='red',markersize=10)\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Tree_Methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Random_Forests.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Decision Trees and Random Forests\n",
    "# https://medium.com/@josemarcialportilla/enchanted-random-forest-b08d418cb411#.q5jgb6hrp\n",
    "# https://www.quora.com/How-does-randomization-in-a-random-forest-work/answer/Edwin-Chen-1?srid=OHjt\n",
    "\n",
    "# Load The Data\n",
    "df = pd.read_csv('kyphosis.csv')  # Age (Months, Children), Number of Vertabae, Vertabrae # surgery started on.\n",
    "\n",
    "# Assign X,y variable\n",
    "X = df.drop('Kyphosis',axis=1)\n",
    "y = df['Kyphosis']  # Variable your trying to predict/Output\n",
    "\n",
    "# Split into Training and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dtree = DecisionTreeClassifier()\n",
    "model_dtree.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_dtree.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_dtree.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Visualize Tree\n",
    "\n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot \n",
    "\n",
    "features = list(df.columns[1:])\n",
    "\n",
    "dot_data = StringIO()  \n",
    "export_graphviz(model_dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph[0].create_png())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Random Forest Classifier (Typically always better than just a single decision tree)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rfc = RandomForestClassifier(n_estimators=200)\n",
    "model_rfc.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_dtree.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_rfc.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs) - Regression and Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=3liCbRZPrZA&list=PLPOToaDS0OPCcFtZZmFArlcSpqtEb1514&index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SVM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Support Vector Call (SVM) - Classifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load The Data\n",
    "dicty_cancer = load_breast_cancer()\n",
    "df_features = pd.DataFrame(dicty_cancer['data'],columns=dicty_cancer['feature_names'])\n",
    "\n",
    "# Assign X,y variable\n",
    "X = df_features\n",
    "y = dicty_cancer['target']  # Variable your trying to predict/Output\n",
    "\n",
    "# Scale The Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = df_X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "# Split into Training and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=101)\n",
    "\n",
    "# GridSearchCV to determine best model paramaters\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}  \n",
    "# Large C = low bias, high variance| Large Gamma = High bias, low variance\n",
    "grid = GridSearchCV(SVC(),param_grid,verbose=3)  # Verbose = higher number, more text output\n",
    "grid.fit(X_train,y_train)\n",
    "grid_best_params = grid.best_params_\n",
    "grid_best_estimator = grid.best_estimator_  # Best model (with params)\n",
    "grid_predictions = grid.predict(X_test)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(C=100,gamma=.0001)  # I got these from grid_best_params. You could just pass in grid.best_estimator_.\n",
    "model_svc.fit(X_train,y_train)\n",
    "# Measure how well fit our model (how good it can predict based on train data)\n",
    "fit_score = model_svc.score(X_test,y_test)  # Returns R^2 between 0 and 1. Higher the better fit.\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_svc.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering Classifier (Unsupervised) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Kmeans.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Kmeans_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0 49]\n",
      " [ 0 50  0  0]\n",
      " [47  0  0  3]\n",
      " [ 2  0 46  2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.02      0.02      0.02        50\n",
      "          1       1.00      1.00      1.00        50\n",
      "          2       0.00      0.00      0.00        50\n",
      "          3       0.04      0.04      0.04        50\n",
      "\n",
      "avg / total       0.26      0.27      0.26       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11b0cf090>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# K-Means Clustering Classifier (Unsupervised)\n",
    "\n",
    "# Load The Data\n",
    "from sklearn.datasets import make_blobs  # Makes artifical scattered data\\\n",
    "tuple_data = make_blobs(n_samples=200,n_features=2,centers=4,cluster_std=1.8,random_state=101)\n",
    "\n",
    "# Assign X,y variable\n",
    "X = pd.DataFrame(tuple_data[0])  # 200x2 array of data points\n",
    "y = tuple_data[1]  # Number for each cluster\n",
    "# Visualize the Data via Scatter Plot\n",
    "plt.scatter(X[0],X[1],c=y,cmap='rainbow')\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.cluster import KMeans\n",
    "model_kmeans = KMeans(n_clusters=4)\n",
    "model_kmeans.fit(X)\n",
    "\n",
    "# Meaure Performance of Kmeans\n",
    "from sklearn import metrics\n",
    "homogeneity = metrics.homogeneity_score(y, model_kmeans.labels_)\n",
    "completeness = metrics.completeness_score(y, model_kmeans.labels_)\n",
    "v_measure = metrics.v_measure_score(y, model_kmeans.labels_)\n",
    "adjusted_rand_index = metrics.adjusted_rand_score(y, model_kmeans.labels_)\n",
    "silhouette_coefficient = metrics.silhouette_score(y, model_kmeans.labels_, sample_size=1000) \n",
    "\n",
    "# (OPTIONAL) Get centroids for clusters\n",
    "cluster_centers = model_kmeans.cluster_centers_\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict/Return y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = model_kmeans.labels_\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y,predictions_values)\n",
    "classification_report = classification_report(y,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)\n",
    "\n",
    "# Scatter Plot Visual\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, sharey=True,figsize=(10,6))\n",
    "\n",
    "ax1.set_title('Actual')\n",
    "ax1.scatter(X[0],X[1],c=y,cmap='rainbow')\n",
    "\n",
    "ax2.set_title('K Means')\n",
    "ax2.scatter(X[0],X[1],c=predictions_values,cmap='rainbow')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heirachale Clustering #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBS Scan #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Prinicipal Component Analysis (Unsupervised) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PCA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Prinicipal Component Analysis (Reducing/Finding most important features)\n",
    "\n",
    "# Load The Data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dicty_cancer = load_breast_cancer()\n",
    "df_features = pd.DataFrame(dicty_cancer['data'],columns=dicty_cancer['feature_names'])\n",
    "\n",
    "# Assign X,y variable\n",
    "X = df_features\n",
    "y = dicty_cancer['target']  # Variable your trying to predict/Output\n",
    "\n",
    "# Scale The Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_scaled = df_X_scaled = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "# Determine Type of Model and Fit It\n",
    "from sklearn.decomposition import PCA\n",
    "model_pca = PCA(n_components=2)  # I got these from grid_best_params. You could just pass in grid.best_estimator_.\n",
    "model_pca.fit(X_scaled)\n",
    "\n",
    "# Create new X with new smaller features (which you can plug into classification algorithm)\n",
    "X_pca = model_pca.transform(X_scaled)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Visualizing PCA\n",
    "\n",
    "# Plot Important Features (to show clear split)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=y,cmap='plasma') # \n",
    "plt.xlabel('First PCA')\n",
    "plt.ylabel('Second PCA')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Heatmap that shows what each PCA is made/correlated with what features\n",
    "pca_components = model_pca.components_\n",
    "df_components = pd.DataFrame(pca_components,columns=X.columns,index=['PCA 1','PCA 2'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_components,cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Truncated SVD + Normalizer (For Sparse Matrix/Text Data)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "svd = TruncatedSVD(300)\n",
    "normalizer = Normalizer(copy=False)\n",
    "pipeline_lsa = make_pipeline(svd, normalizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Recommender_Systems.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Recommender Systems (NEEDS FIXING)\n",
    "\n",
    "# Load The Data\n",
    "columns_names = ['user_id','item_id','rating','timestamp']\n",
    "df = pd.read_csv('u.data',sep='\\t',names=columns_names)\n",
    "\n",
    "# Add Movie Titles\n",
    "movie_titles = pd.read_csv('Movie_Id_Titles')\n",
    "df = pd.merge(df,movie_titles,on='item_id')\n",
    "\n",
    "# Create DF with title, avg_rating, and # of ratings\n",
    "df_ratings = pd.DataFrame(df.groupby('title')['rating'].mean())\n",
    "df_ratings['Number_of_Ratings'] = pd.DataFrame(df.groupby('title')['rating'].count())\n",
    "\n",
    "# Create matrix with user_id x Title\n",
    "matrix_movie = df.pivot_table(index='user_id',columns='title',values='rating')\n",
    "\n",
    "# Get correlation with popular movies and all movies\n",
    "starwars_user_ratings = matrix_movie['Star Kid (1997)']\n",
    "liarliar_user_ratings = matrix_movie['Liar Liar (1997)']\n",
    "\n",
    "# Create Similar to Variables\n",
    "similar_to_starwars = matrix_movie.corrwith(starwars_user_ratings)\n",
    "similar_to_liarliar = matrix_movie.corrwith(liarliar_user_ratings)\n",
    "\n",
    "# Create dataframe with movies and their correlation to starwars (filtering out movies that don't have a lot of ratings)\n",
    "df_corr_starwars = pd.DataFrame(similar_to_starwars,columns=['Correlation'])\n",
    "df_corr_starwars.dropna(inplace=True)\n",
    "df_corr_starwars = df_corr_starwars.join(df_corr_starwars['num of ratings'])\n",
    "df_corr_starwars[df_corr_starwars['num of ratings']>100].sort_values('Correlation',ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://www.astroml.org/sklearn_tutorial/_images/plot_ML_flow_chart_3.png' width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NLP_Bag_Of_Words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Natural Language Processing (NLP)\n",
    "# Need to conda install nltk first\n",
    "\n",
    "import nltk\n",
    "# nltk.download_shell()  # Type d, enter, stopwords, q.\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Load The Data & Create X,y Variables\n",
    "\n",
    "# Load The Data\n",
    "df_text_messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names=['label','message'])\n",
    "\n",
    "# Create X,y variables\n",
    "X = df_text_messages['message']\n",
    "y = df_text_messages['label']\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Split into Training and Testing Set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Filter for Vectorizer (Custom or Vecotrizer Arguments)\n",
    "\n",
    "# Remove punctioation and stop words from messages and create list of words for each message\n",
    "\n",
    "# Custom Filter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "def custom_filter(message):\n",
    "    nopunc = [char for char in message if char not in string.punctuation]\n",
    "    nopunc = \"\".join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "df_text_messages['message'] = df_text_messages['message'].apply(custom_filter)\n",
    "\n",
    "# TFIDF Arguments\n",
    "vectorizer_tfidf = TfidfVectorizer(max_df=.5, max_features=1000,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=True, sublinear_tf=False, \n",
    "                                 analyzer = 'word',\n",
    "                                 token_pattern = '[a-zA-Z]{4,50}',\n",
    "                            )\n",
    "\n",
    "# CountVectorizer Arguments\n",
    "vectorizer_count = CountVectorizer(stop_words = 'english', ngram_range = (1,2), max_features=1000)\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Vector for Each Message (2x2 Sparse Matrix of Word Counts where columns = messages, rows word 1 count, word 2 count, etc)\n",
    "                       #This Image Helps Understand = <img src=\"Count_Vectorizer.png\">#\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "bow_transformer = CountVectorizer(analyzer=customer_filter).fit(X)\n",
    "df_text_messages_bow = bow_transformer.transform(X)  # Creates df of count vectors\n",
    "\n",
    "# Create TfidF Transformer (Returns weight value instead of count of word)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(df_text_messages_bow)\n",
    "df_text_messages_bow = bow_transformer.transform(df_text_messages['message'])  # Creates df of tfidf vectors\n",
    "df_text_messages_tfidf = tfidf_transformer.transform(df_text_messages_bow)\n",
    "\n",
    "# Good Info (OPTIONAL)\n",
    "# tfidf_frequency_university = tfidf_transformer.idf_[bow_transformer.vocabulary_['university']]\n",
    "unique_words = bow_transformer.vocabulary_  \n",
    "sparsity_measure = sparsity = (100.0 * df_text_messages_bow.nnz / (df_text_messages_bow.shape[0] * df_text_messages_bow.shape[1]))  # Non-Zero Messages/Total Messages\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Create Pipeline (Performs/Replaces The Above Steps)\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "        ('bow',CountVectorizer(analyzer=customer_filter)),  # Turns each message into 2x2 Sparse Matrix where columns = messages, rows word 1 count, word 2 count, etc)\n",
    "        ('tfidf',TfidfTransformer()),  # Turns bag of word counts into weight values\n",
    "        ('classifier',MultinomialNB())  # Uses weight values to create/fit model\n",
    "        ])\n",
    "\n",
    "pipeline.fit(X_train,y_train)  # Pass in text data and it runs the pipeline on it\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Predict y Values\n",
    "\n",
    "# Returns raw label/number value\n",
    "predictions_values = pipeline.predict(X_test)\n",
    "# Returns probablity of that row having the categorical label\n",
    "# predictions_probability = model_lm.predict_proba(X_test)\n",
    "\n",
    "# Measure Accuracy of Predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,predictions_values)\n",
    "classification_report = classification_report(y_test,predictions_values)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cat_Naive_Bayes = naive_bayes.MultinomialNB();\n",
    "Cat_Naive_Bayes.fit(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('../../assets/datasets/rossmann.csv', skipinitialspace=True, low_memory=False)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "data['Year'] = data.index.year\n",
    "data['Month'] = data.index.month\n",
    "\n",
    "# Autocorrelation\n",
    "data['Sales'].resample('D', how='mean').autocorr(lag=1)\n",
    "\n",
    "# Rolling Average\n",
    "data[['Sales']].resample('M', how=['median', 'mean']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futures #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    " \n",
    "def return_after_3_secs(message):\n",
    "    sleep(3)\n",
    "    print 'here'\n",
    "    return message\n",
    "\n",
    "# There are three threads here!\n",
    "pool = ThreadPoolExecutor(3)\n",
    " \n",
    "future = pool.submit(return_after_3_secs, ('fin'))\n",
    "print(future.done())\n",
    "sleep(3)\n",
    "print(future.done())\n",
    "print 'there'\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "    \n",
    "URLS = ['http://www.foxnews.com/',\n",
    "        'http://www.cnn.com/',\n",
    "        'http://europe.wsj.com/',\n",
    "        'http://www.bbc.co.uk/',\n",
    "        'http://google.com/']\n",
    " \n",
    "# Retrieve a single page and report the url and contents\n",
    "def load_url(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    " \n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, url): url for url in URLS}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "        else:\n",
    "            print('%r page is %d bytes' % (url, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Present The Results #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data (Hadoop, Hive, Spark, mrjob, AWS, etc) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Local_vs_Distributed.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hadoop #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Hadoop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Map_Reduce.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark (flexible/faster alternative to Map Reduce by keeping in memory vs wrirting to disk) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Spark.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Spark_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Spark_3.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step By Step Guide To Setup Spark on Jupyter Notebook: https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://spark.apache.org/docs/latest/programming-guide.html\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Creates RDD\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "\n",
    "%%writefile example.txt\n",
    "first\n",
    "second line\n",
    "the third line\n",
    "then a fourth line\n",
    "\n",
    "%%writefile services.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "206       10/19/2017      202       CA       131          500.00\n",
    "203       10/17/2017      101       NY       173          750.00\n",
    "205       10/19/2017      202       TX       121          200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Spark Functions\n",
    "\n",
    "# TRANSFORMATIONS (transforms RDD)\n",
    "\n",
    "# Assign RDD\n",
    "RDD_textFile = sc.textFile('example.txt')\n",
    "RDD_services = sc.textFile('services.txt')\n",
    "\n",
    "# ACTIONS (produces object)\n",
    "\n",
    "# Count number of lines\n",
    "RDD_textFile.count()\n",
    "# First Line\n",
    "RDD_textFile.first()\n",
    "# Filter\n",
    "secfind = RDD_textFile.filter(lambda line: 'second' in line)  # Find the line with 'second in it'\n",
    "# Map + Split\n",
    "words = RDD_textFile.map(lambda line: line.split())  # Returns every word per line in seperate list\n",
    "words2 = RDD_textFile  # Returns all lines in single list\n",
    "# Flat Map\n",
    "words3 = RDD_textFile.flatMap(lambda line: line.split()) # Returns all lines in single list\n",
    "# Take\n",
    "RDD_services.take(2)  # Returns first two lines\n",
    "RDD_services.map(lambda line: line.split()).take(3)  # Returns first 3 lines with each line in seperate list\n",
    "\n",
    "# Slicing/Cleaning\n",
    "clean = RDD_services.map(lambda line: line[1:] if line[0]=='#' else line)  # Removes the '#' from the column name\n",
    "clean = clean.map(lambda line: line.split())  # (Use .collect() after in test)\n",
    "\n",
    "# Reduce By Key (Group By, must take in list of key, value pair tuples)\n",
    "step1_pairs = clean.map(lambda listy: (listy[3],listy[-1]))\n",
    "step2_rekey = step1_pairs.reduceByKey(lambda amt1,amt2 : float(amt1) + float(amt2))  # Use .collect() to test\n",
    "step3_clean = step2_rekey.filter(lambda x: not x[0]=='State')  # Get rid of \"(State, Amount)\" tuple\n",
    "step4_sort = step3_clean.sortBy(lambda amt: amt[1],ascending=False)\n",
    "step5_action = step4_sort.collect()\n",
    "\n",
    "# Functions\n",
    "def step1_pairs(id_state_amt):  # Replaces step1_pairs above\n",
    "    # Tuple Unpacking\n",
    "    (Id,state,amt) = id_state_amt\n",
    "    return amt\n",
    "    \n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# ACTION TEST HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS #"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Command Line To Compute On Cloud #\n",
    "______________________________________________________________________________________________________________________\n",
    "chmod 400 new_spark.pem \n",
    "ssh -i new_spark.pem ubuntu@ec2-54-89-220-40.compute-1.amazonaws.com\n",
    "ec2-user\n",
    "______________________________________________________________________________________________________________________\n",
    "# How to get Jupyter Notebook and Spark Running on AWS #\n",
    "______________________________________________________________________________________________________________________\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297#.9h6ywo6b9\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78750529,  0.23111785,  0.63283989,  0.75511507,  0.97768313])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Random Number Generator\n",
    "\n",
    "# Python Default\n",
    "import random\n",
    "random1 = random.randint(0,100) \n",
    "random2 = random.randrange(100,step=2)\n",
    "random3 = random.choice([1,2,3,4,5])  # Can't use on dictionary\n",
    "#Numpy\n",
    "import numpy as np\n",
    "random4 = np.random.randint(0,100,size=(100,10))  # Size is optional\n",
    "random5 = random.sample(xrange(100), 10)  # Random list of 10 numbers\n",
    "random6 = np.random.rand(5)  # Vector with random uniform distributed numbers from 0-1\n",
    "random7 = np.random.randn(5,5)  # 5x5 Matrix with random normal distributed numbers from 0-1\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Misc\n",
    "from string import ascii_lowercase\n",
    "lc_letters = list(ascii_lowercase)\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________________________#\n",
    "# Test Here\n",
    "random6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new environement in command line while in project folder\n",
    "source .venv/bin/activate\n",
    "# From Scratch\n",
    "conda create --name mypthon3version python=3.5 numpy\n",
    "source activate mypthon3version\n",
    "source deactivate mypthon3version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get description of dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dicty_cancer = load_breast_cancer()\n",
    "print(dicty_cancer['DESCR'])\n",
    "# Makes artifical scattered data\n",
    "from sklearn.datasets import make_blobs\n",
    "tuple_data = make_blobs(n_samples=200,n_features=2,centers=4,cluster_std=1.8,random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of punctiuations, english stopwords, etc\n",
    "import string\n",
    "string.punctuation\n",
    "from nlt.corpus import stopwords\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create text files\n",
    "\n",
    "%%writefile services.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "201       10/13/2017      100       NY       131          100.00\n",
    "204       10/18/2017      700       TX       129          450.00\n",
    "202       10/15/2017      203       CA       121          200.00\n",
    "206       10/19/2017      202       CA       131          500.00\n",
    "203       10/17/2017      101       NY       173          750.00\n",
    "205       10/19/2017      202       TX       121          200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_____________________________________________________________________________________________________________________#\n",
    "# Twitter API\n",
    "# https://apps.twitter.com/app/13123126/keys\n",
    "# https://github.com/ryanmcgrath/twython\n",
    "from twython import Twython\n",
    "\n",
    "TWITTER_APP_KEY = 'tDTFnA5M7R4pxtaCMue7nyGbU'\n",
    "TWITTER_APP_SECRET = '6oKhP5vgQuWAMiYLjjHOllvjkp4H7GeYB4QILAq3iOhALu8kiw'\n",
    "TWITTER_OAUTH_TOKEN = '1516857505-sG8bgxN2RKYzC3VfBQJawc70RdIGlfQ8TEbP2kb'\n",
    "TWITTER_OAUTH_TOKEN_SECRET = 'VvEXhQJj2ZISpcnLQgHHAC7mArwcpFR3HweSW6eIhNaVx'\n",
    "\n",
    "twitter = Twython(TWITTER_APP_KEY, TWITTER_APP_SECRET,TWITTER_OAUTH_TOKEN,TWITTER_OAUTH_TOKEN_SECRET)\n",
    "\n",
    "results = twitter.search(q='seahawks', result_type ='recent')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find Hidden Files = ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Helpful Charts #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outstanding To-Dos #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
